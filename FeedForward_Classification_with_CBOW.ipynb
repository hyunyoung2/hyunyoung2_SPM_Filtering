{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, The file(/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_HAMvector ) read\n",
      "Currently, The file(/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_SPMVector ) read\n",
      "Currently, The file(/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_HAMVector ) read\n",
      "Currently, The file(/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_SPMVector ) read\n",
      "Importing data.... it is done!\n",
      "len of trainHAMX 49993 type of trainHAMX <class 'numpy.ndarray'> shape of trainHAMX (49993, 300)\n",
      "len of trainSPMX 50000 type of trainSPMX <class 'numpy.ndarray'> shape of trainSPMX (50000, 300)\n",
      "len of testHAMX 5000 type of testHAMX <class 'numpy.ndarray'> shape of testHAMX (5000, 300)\n",
      "len of testSPMX 5000 type of testSPMX <class 'numpy.ndarray'> shape of testSPMX (5000, 300)\n",
      "\n",
      "len of trainHAMY 49993 type of trainHAMY <class 'numpy.ndarray'> shape of trainHAMY (49993, 2)\n",
      "len of trainSPMY 50000 type of trainSPMY <class 'numpy.ndarray'> shape of trainSPMY (50000, 2)\n",
      "len of testHAMY 5000 type of testHAMY <class 'numpy.ndarray'> shape of testHAMY (5000, 2)\n",
      "len of testSPMY 5000 type of testSPMY <class 'numpy.ndarray'> shape of testSPMY (5000, 2)\n",
      "\n",
      "[[ -3.40029000e-01  -1.14786000e-01  -1.42400000e-03 ...,  -4.27502000e-01\n",
      "   -2.18332000e-01   6.32502000e-01]\n",
      " [ -3.00340400e+00   1.89925200e+00   2.46149400e+00 ...,  -1.35812500e+00\n",
      "    5.89687200e+00   3.51193900e+00]\n",
      " [ -2.97875900e+00   2.31656800e+00   2.61098400e+00 ...,  -1.32839400e+00\n",
      "    5.79280600e+00   3.76249100e+00]\n",
      " ..., \n",
      " [ -1.41147900e+00  -1.92260600e+00  -2.39580000e-02 ...,   2.92853800e+00\n",
      "    2.21546500e+00  -7.79321000e-01]\n",
      " [ -3.57697000e-01   3.11499000e-01   1.73708900e+00 ...,   6.35290000e-02\n",
      "   -1.08394200e+00   5.20960000e-01]\n",
      " [  1.72065000e-01   1.74693300e+00  -1.98533000e+00 ...,  -3.55330000e-02\n",
      "   -2.71601200e+00   1.68331100e+00]]\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n",
      "Finally, basic setting of data is done!!\n",
      "len of trainX 99993 type of trainX <class 'numpy.ndarray'> shape of trainX (99993, 300)\n",
      "len of testX 10000 type of testX <class 'numpy.ndarray'> shape of testX (10000, 300)\n",
      "len of trainY 99993 type of trainY <class 'numpy.ndarray'> shape of trainY (99993, 2)\n",
      "len of testY 10000 type of testY <class 'numpy.ndarray'> shape of trainY (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "# CBOW_HAMvector CBOW_SPMVector  SKIP_GRAM_HAMvector  SKIP_GRAM_SPMVector\n",
    "CBOWTrain = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_HAMvector\",\n",
    "            \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_SPMVector\",]\n",
    "\n",
    "SKIPGRAMTrain = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_HAMvector\",\n",
    "                \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_SPMVector\"]\n",
    "\n",
    "# CBOW_HAMVector  CBOW_SPMVector  SKIP_GRAM_HAMVector  SKIP_GRAM_SPMVector\n",
    "CBOWTest = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_HAMVector\",\n",
    "          \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_SPMVector\",]\n",
    "\n",
    "\n",
    "SKIPGRAMTest = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_HAMVector\",\n",
    "               \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_SPMVector\"]\n",
    "\n",
    "\n",
    "# 0 CBOW, 1 SKIPGRAM\n",
    "VectorType = 0\n",
    "\n",
    "\n",
    "def tsvToNumpyArr(file, delimiterOfDoc):\n",
    "    print (\"Currently, The file(\" + file, \") read\")\n",
    "    return np.genfromtxt(file, delimiter=delimiterOfDoc, dtype=float)\n",
    "\n",
    "def importCBOWData():\n",
    "    CBOWTrainHAM = tsvToNumpyArr(CBOWTrain[0], delimiterOfDoc=\"\\t\")\n",
    "    CBOWtrainSPM = tsvToNumpyArr(CBOWTrain[1], delimiterOfDoc=\"\\t\")\n",
    "    CBOWtestHAM = tsvToNumpyArr(CBOWTest[0], delimiterOfDoc=\"\\t\")\n",
    "    CBOWtestSPM = tsvToNumpyArr(CBOWTest[1], delimiterOfDoc=\"\\t\")\n",
    "    return CBOWTrainHAM, CBOWtrainSPM, CBOWtestHAM, CBOWtestSPM\n",
    "    \n",
    "def importSKIPGRAMData():\n",
    "    SKIPGRAMTrainHAM = tsvToNumpyArr(SKIPGRAMTrain[0], delimiterOfDoc=\"\\t\")\n",
    "    SKIPGRAMtrainSPM = tsvToNumpyArr(SKIPGRAMTrain[1], delimiterOfDoc=\"\\t\")\n",
    "    SKIPGRAMtestHAM = tsvToNumpyArr(SKIPGRAMTest[0], delimiterOfDoc=\"\\t\")\n",
    "    SKIPGRAMtestSPM = tsvToNumpyArr(SKIPGRAMTest[1], delimiterOfDoc=\"\\t\")\n",
    "    return SKIPGRAMTrainHAM, SKIPGRAMtrainSPM, SKIPGRAMtestHAM, SKIPGRAMtestSPM\n",
    "\n",
    "\n",
    "def importData():\n",
    "    if VectorType == 0:\n",
    "        CBOWTrainHAM, CBOWtrainSPM, CBOWtestHAM, CBOWtestSPM = importCBOWData()\n",
    "        return CBOWTrainHAM, CBOWtrainSPM, CBOWtestHAM, CBOWtestSPM\n",
    "    else:\n",
    "        SKIPGRAMTrainHAM, SKIPGRAMtrainSPM, SKIPGRAMtestHAM, SKIPGRAMtestSPM = importSKIPGRAMData()\n",
    "        return SKIPGRAMTrainHAM, SKIPGRAMtrainSPM, SKIPGRAMtestHAM, SKIPGRAMtestSPM\n",
    "    \n",
    "trainHAMX, trainSPMX, testHAMX, testSPMX = importData()\n",
    "\n",
    "print(\"Importing data.... it is done!\")\n",
    "print(\"len of trainHAMX\", len(trainHAMX), \"type of trainHAMX\", type(trainHAMX), \"shape of trainHAMX\", trainHAMX.shape)\n",
    "print(\"len of trainSPMX\", len(trainSPMX), \"type of trainSPMX\", type(trainSPMX), \"shape of trainSPMX\", trainSPMX.shape)\n",
    "print(\"len of testHAMX\", len(testHAMX), \"type of testHAMX\", type(testHAMX), \"shape of testHAMX\", testHAMX.shape)\n",
    "print(\"len of testSPMX\", len(testSPMX), \"type of testSPMX\", type(testSPMX), \"shape of testSPMX\", testSPMX.shape)\n",
    "print()\n",
    "\n",
    "def zerosArr(arr):\n",
    "    return np.zeros((arr.shape[0],1), dtype=float)\n",
    "\n",
    "def onesArr(arr):\n",
    "    return np.ones((arr.shape[0],1), dtype=float)\n",
    "\n",
    "def generateLabel(trainHAMX, trainSPMX, testHAMX, testSPMX):\n",
    "    trainHAMY = np.concatenate((onesArr(trainHAMX), zerosArr(trainHAMX)), axis=1)\n",
    "    trainSPMY = np.concatenate((zerosArr(trainSPMX), onesArr(trainSPMX)), axis=1)\n",
    "    testHAMY = np.concatenate((onesArr(testHAMX), zerosArr(testHAMX)), axis=1)\n",
    "    testSPMY = np.concatenate((zerosArr(testSPMX), onesArr(testSPMX)), axis=1)\n",
    "    \n",
    "    return trainHAMY, trainSPMY, testHAMY, testSPMY\n",
    "    \n",
    "trainHAMY, trainSPMY, testHAMY, testSPMY = generateLabel(trainHAMX, trainSPMX, testHAMX, testSPMX)\n",
    "\n",
    "print(\"len of trainHAMY\", len(trainHAMY), \"type of trainHAMY\", type(trainHAMY), \"shape of trainHAMY\", trainHAMY.shape)\n",
    "print(\"len of trainSPMY\", len(trainSPMY), \"type of trainSPMY\", type(trainSPMY), \"shape of trainSPMY\", trainSPMY.shape)\n",
    "print(\"len of testHAMY\", len(testHAMY), \"type of testHAMY\", type(testHAMY), \"shape of testHAMY\", testHAMY.shape)\n",
    "print(\"len of testSPMY\", len(testSPMY), \"type of testSPMY\", type(testSPMY), \"shape of testSPMY\", testSPMY.shape)\n",
    "print()\n",
    "\n",
    "def concatenateData(trainHAMX, trainSPMX, testHAMX, testSPMX):\n",
    "    trainX = np.concatenate((trainHAMX, trainSPMX), axis=0)\n",
    "    testX = np.concatenate((testHAMX, testSPMX), axis=0)\n",
    "    \n",
    "    return trainX, testX\n",
    "\n",
    "\n",
    "def concatenateLabel(trainHAMY, trainSPMY, testHAMY, testSPMY):\n",
    "    trainY = np.concatenate((trainHAMY, trainSPMY), axis=0)\n",
    "    testY = np.concatenate((testHAMY, testSPMY), axis=0)\n",
    "    \n",
    "    return trainY, testY\n",
    "\n",
    "\n",
    "\n",
    "trainX, testX = concatenateData(trainHAMX, trainSPMX, testHAMX, testSPMX)\n",
    "trainY, testY = concatenateLabel(trainHAMY, trainSPMY, testHAMY, testSPMY)\n",
    "\n",
    "print(trainX)\n",
    "print(trainY)\n",
    "\n",
    "print(\"Finally, basic setting of data is done!!\")\n",
    "\n",
    "print(\"len of trainX\", len(trainX), \"type of trainX\", type(trainX), \"shape of trainX\", trainX.shape)\n",
    "print(\"len of testX\", len(testX), \"type of testX\", type(testX), \"shape of testX\", testX.shape)\n",
    "print(\"len of trainY\", len(trainY), \"type of trainY\", type(trainY), \"shape of trainY\", trainY.shape)\n",
    "print(\"len of testY\", len(testY), \"type of testY\", type(testY), \"shape of trainY\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/27000 [00:00<1:52:14,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.537618\n",
      "step 0, train_cost 1.54031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1002/27000 [02:09<56:12,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, training accuracy 0.753623\n",
      "step 1000, train_cost 0.531973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2002/27000 [04:21<54:20,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 0.768984\n",
      "step 2000, train_cost 0.496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 3002/27000 [06:29<51:52,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000, training accuracy 0.775464\n",
      "step 3000, train_cost 0.478034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 4003/27000 [08:36<49:26,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, training accuracy 0.780235\n",
      "step 4000, train_cost 0.466908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5003/27000 [09:59<43:54,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000, training accuracy 0.783035\n",
      "step 5000, train_cost 0.45929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 6003/27000 [11:18<39:31,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, training accuracy 0.785515\n",
      "step 6000, train_cost 0.453728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 7003/27000 [12:35<35:57,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000, training accuracy 0.787535\n",
      "step 7000, train_cost 0.449488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8003/27000 [13:53<32:58,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 0.790025\n",
      "step 8000, train_cost 0.446165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9003/27000 [15:08<30:16,  9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000, training accuracy 0.792045\n",
      "step 9000, train_cost 0.443527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10003/27000 [16:23<27:51, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, training accuracy 0.793896\n",
      "step 10000, train_cost 0.441403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 11003/27000 [17:39<25:39, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11000, training accuracy 0.795266\n",
      "step 11000, train_cost 0.439647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12003/27000 [18:54<23:37, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, training accuracy 0.796476\n",
      "step 12000, train_cost 0.438165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 13003/27000 [20:17<21:50, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13000, training accuracy 0.797446\n",
      "step 13000, train_cost 0.436898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 14003/27000 [21:40<20:07, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, training accuracy 0.798246\n",
      "step 14000, train_cost 0.4358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 15003/27000 [23:04<18:26, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15000, training accuracy 0.799216\n",
      "step 15000, train_cost 0.434839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 16003/27000 [24:27<16:48, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 0.800346\n",
      "step 16000, train_cost 0.433988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 17003/27000 [25:44<15:08, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17000, training accuracy 0.800816\n",
      "step 17000, train_cost 0.433228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 18003/27000 [27:03<13:31, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18000, training accuracy 0.801216\n",
      "step 18000, train_cost 0.432544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 19003/27000 [28:26<11:58, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19000, training accuracy 0.801696\n",
      "step 19000, train_cost 0.431923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 20003/27000 [29:50<10:26, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20000, training accuracy 0.802176\n",
      "step 20000, train_cost 0.431355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 21003/27000 [31:13<08:54, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21000, training accuracy 0.802776\n",
      "step 21000, train_cost 0.430832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 22003/27000 [32:37<07:24, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22000, training accuracy 0.803126\n",
      "step 22000, train_cost 0.430348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 23003/27000 [33:57<05:53, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23000, training accuracy 0.803416\n",
      "step 23000, train_cost 0.429898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 24003/27000 [35:18<04:24, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24000, training accuracy 0.803646\n",
      "step 24000, train_cost 0.429478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 25003/27000 [36:42<02:55, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25000, training accuracy 0.804086\n",
      "step 25000, train_cost 0.429083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 26003/27000 [38:06<01:27, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26000, training accuracy 0.804886\n",
      "step 26000, train_cost 0.42871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [39:29<00:00, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy on Test set: 0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer1\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"Layer1_weight_h1\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"Layer1_bias_h1\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    output = tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"])\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/27000 [00:00<6:44:01,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.581331\n",
      "step 0, train_cost 0.687738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1001/27000 [06:52<2:58:29,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, training accuracy 0.761123\n",
      "step 1000, train_cost 0.532267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2001/27000 [13:50<2:52:49,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 0.774264\n",
      "step 2000, train_cost 0.490933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 3001/27000 [20:10<2:41:24,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000, training accuracy 0.785885\n",
      "step 3000, train_cost 0.469849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 4001/27000 [26:22<2:31:37,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, training accuracy 0.788395\n",
      "step 4000, train_cost 0.456878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5001/27000 [32:29<2:22:57,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000, training accuracy 0.795536\n",
      "step 5000, train_cost 0.448075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 6001/27000 [38:31<2:14:47,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, training accuracy 0.797946\n",
      "step 6000, train_cost 0.441615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 7001/27000 [44:39<2:07:35,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000, training accuracy 0.802606\n",
      "step 7000, train_cost 0.436568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8001/27000 [50:50<2:00:42,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 0.803886\n",
      "step 8000, train_cost 0.432434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9001/27000 [56:56<1:53:52,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000, training accuracy 0.804686\n",
      "step 9000, train_cost 0.428923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10001/27000 [1:03:06<1:47:15,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, training accuracy 0.805266\n",
      "step 10000, train_cost 0.42586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 11001/27000 [1:09:10<1:40:36,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11000, training accuracy 0.806416\n",
      "step 11000, train_cost 0.423135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12001/27000 [1:15:10<1:33:57,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, training accuracy 0.807557\n",
      "step 12000, train_cost 0.420676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 13001/27000 [1:21:10<1:27:24,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13000, training accuracy 0.808067\n",
      "step 13000, train_cost 0.418437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 14001/27000 [1:27:10<1:20:56,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, training accuracy 0.808547\n",
      "step 14000, train_cost 0.416382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 15001/27000 [1:33:10<1:14:31,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15000, training accuracy 0.809267\n",
      "step 15000, train_cost 0.414479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 16001/27000 [1:39:10<1:08:10,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 0.809747\n",
      "step 16000, train_cost 0.412704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 17001/27000 [1:45:12<1:01:52,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17000, training accuracy 0.810447\n",
      "step 17000, train_cost 0.41104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 18001/27000 [1:51:28<55:43,  2.69it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18000, training accuracy 0.810847\n",
      "step 18000, train_cost 0.409474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 19001/27000 [1:57:41<49:32,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19000, training accuracy 0.811367\n",
      "step 19000, train_cost 0.407994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 20001/27000 [2:03:54<43:21,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20000, training accuracy 0.812407\n",
      "step 20000, train_cost 0.40659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 21001/27000 [2:10:30<37:16,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21000, training accuracy 0.812677\n",
      "step 21000, train_cost 0.405256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 22001/27000 [2:16:48<31:05,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22000, training accuracy 0.813457\n",
      "step 22000, train_cost 0.403984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 23001/27000 [2:23:04<24:52,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23000, training accuracy 0.813947\n",
      "step 23000, train_cost 0.402769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 24001/27000 [2:29:25<18:40,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24000, training accuracy 0.814407\n",
      "step 24000, train_cost 0.401606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 25001/27000 [2:35:44<12:27,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25000, training accuracy 0.815087\n",
      "step 25000, train_cost 0.400491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 26001/27000 [2:42:03<06:13,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26000, training accuracy 0.815497\n",
      "step 26000, train_cost 0.39942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [2:48:03<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy on Test set: 0.9586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer2\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"Layer2_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"Layer2_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"Layer2_bias_h1\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"Layer2_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    output = tf.add(tf.matmul(layer1, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/27000 [00:01<8:09:32,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.499965\n",
      "step 0, train_cost 1.13912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1001/27000 [11:08<4:49:29,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, training accuracy 0.695389\n",
      "step 1000, train_cost 0.668288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2001/27000 [22:16<4:38:12,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 0.745602\n",
      "step 2000, train_cost 0.639187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 3001/27000 [33:04<4:24:30,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000, training accuracy 0.761893\n",
      "step 3000, train_cost 0.613517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 4001/27000 [43:55<4:12:31,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, training accuracy 0.768634\n",
      "step 4000, train_cost 0.590449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5001/27000 [54:53<4:01:28,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000, training accuracy 0.769764\n",
      "step 5000, train_cost 0.569795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 6001/27000 [1:05:52<3:50:31,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, training accuracy 0.771494\n",
      "step 6000, train_cost 0.551539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 7001/27000 [1:16:51<3:39:34,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000, training accuracy 0.773254\n",
      "step 7000, train_cost 0.535626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8001/27000 [1:27:48<3:28:30,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 0.775204\n",
      "step 8000, train_cost 0.521883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9001/27000 [1:38:48<3:17:35,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000, training accuracy 0.776364\n",
      "step 9000, train_cost 0.510046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10001/27000 [1:49:43<3:06:30,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, training accuracy 0.777464\n",
      "step 10000, train_cost 0.499824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 11001/27000 [2:00:42<2:55:33,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11000, training accuracy 0.778134\n",
      "step 11000, train_cost 0.490957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12001/27000 [2:11:36<2:44:28,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, training accuracy 0.778695\n",
      "step 12000, train_cost 0.483219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 13001/27000 [2:22:54<2:33:52,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13000, training accuracy 0.779605\n",
      "step 13000, train_cost 0.476434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 14001/27000 [2:34:30<2:23:26,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, training accuracy 0.780365\n",
      "step 14000, train_cost 0.470462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 15001/27000 [2:46:09<2:12:54,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15000, training accuracy 0.780855\n",
      "step 15000, train_cost 0.465194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 16001/27000 [2:57:38<2:02:06,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 0.781345\n",
      "step 16000, train_cost 0.46054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 17001/27000 [3:09:24<1:51:24,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17000, training accuracy 0.781915\n",
      "step 17000, train_cost 0.45642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 18001/27000 [3:21:03<1:40:30,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18000, training accuracy 0.789855\n",
      "step 18000, train_cost 0.452761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 19001/27000 [3:32:36<1:29:30,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19000, training accuracy 0.790115\n",
      "step 19000, train_cost 0.449497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 20001/27000 [3:44:03<1:18:24,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20000, training accuracy 0.795496\n",
      "step 20000, train_cost 0.446569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 21001/27000 [3:55:32<1:07:16,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21000, training accuracy 0.798776\n",
      "step 21000, train_cost 0.44393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 22001/27000 [4:07:17<56:11,  1.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22000, training accuracy 0.799396\n",
      "step 22000, train_cost 0.441537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 23001/27000 [4:18:46<44:59,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23000, training accuracy 0.799716\n",
      "step 23000, train_cost 0.439357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 24001/27000 [4:30:11<33:45,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24000, training accuracy 0.800436\n",
      "step 24000, train_cost 0.437354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 25001/27000 [4:41:38<22:31,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25000, training accuracy 0.800786\n",
      "step 25000, train_cost 0.435503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 26001/27000 [4:53:06<11:15,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26000, training accuracy 0.801166\n",
      "step 26000, train_cost 0.433779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [5:04:30<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy on Test set: 0.9575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer3\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"Layer3_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"Layer3_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"Layer3_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"Layer3_bias_h1\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"Layer3_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"Layer3_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    output = tf.add(tf.matmul(layer2, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable Layer3_weight_h1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-2-232d30dae439>\", line 17, in <module>\n    weight = { \"h1\" : tf.get_variable(\"Layer3_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-232d30dae439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mground_truths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABELSIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m weight = { \"h1\" : tf.get_variable(\"Layer3_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n\u001b[0m\u001b[1;32m     18\u001b[0m           \u001b[0;34m\"h2\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Layer3_weight_h2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHIDDENLAYERSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDENLAYERSIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m          \"out\": tf.get_variable(\"Layer3_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable Layer3_weight_h1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-2-232d30dae439>\", line 17, in <module>\n    weight = { \"h1\" : tf.get_variable(\"Layer3_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# epoch 35000\n",
    "# Hyper parameter\n",
    "EPOCH = 35000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer3\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"Layer3_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"Layer3_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"Layer3_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"Layer3_bias_h1\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"Layer3_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"Layer3_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    output = tf.add(tf.matmul(layer2, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 35000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer4\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"layer4_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"layer4_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h3\" : tf.get_variable(\"layer4_weight_h3\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"layer4_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"layer4_bias_h1\", [1,HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"layer4_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h3\" : tf.get_variable(\"layer4_bias_h3\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"layer4_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weight[\"h3\"]), biases[\"h3\"]))\n",
    "    output = tf.add(tf.matmul(layer3, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/27000 [00:01<11:28:34,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.499965\n",
      "step 0, train_cost 0.705944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1001/27000 [15:45<6:49:26,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, training accuracy 0.545018\n",
      "step 1000, train_cost 0.691574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2001/27000 [31:21<6:31:46,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 0.636525\n",
      "step 2000, train_cost 0.689056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 3001/27000 [47:28<6:19:42,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000, training accuracy 0.684518\n",
      "step 3000, train_cost 0.686512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 4001/27000 [1:03:27<6:04:47,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, training accuracy 0.70917\n",
      "step 4000, train_cost 0.683903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5001/27000 [1:19:26<5:49:28,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000, training accuracy 0.730521\n",
      "step 5000, train_cost 0.681189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 6001/27000 [1:35:38<5:34:40,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, training accuracy 0.741342\n",
      "step 6000, train_cost 0.678331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 7001/27000 [1:51:49<5:19:26,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000, training accuracy 0.752673\n",
      "step 7000, train_cost 0.675288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8001/27000 [2:08:00<5:03:58,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 0.757783\n",
      "step 8000, train_cost 0.672016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9001/27000 [2:24:09<4:48:17,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000, training accuracy 0.760783\n",
      "step 9000, train_cost 0.668469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10001/27000 [2:40:16<4:32:24,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, training accuracy 0.763133\n",
      "step 10000, train_cost 0.664599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 11001/27000 [2:56:22<4:16:29,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11000, training accuracy 0.764774\n",
      "step 11000, train_cost 0.660355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12001/27000 [3:12:13<4:00:14,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, training accuracy 0.766004\n",
      "step 12000, train_cost 0.655683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 13001/27000 [3:27:59<3:43:57,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13000, training accuracy 0.767504\n",
      "step 13000, train_cost 0.650531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 14001/27000 [3:44:04<3:28:02,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, training accuracy 0.768254\n",
      "step 14000, train_cost 0.644847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 15001/27000 [4:00:01<3:11:59,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15000, training accuracy 0.770154\n",
      "step 15000, train_cost 0.638584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 16001/27000 [4:15:37<2:55:43,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 0.770514\n",
      "step 16000, train_cost 0.631706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 17001/27000 [4:31:14<2:39:31,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17000, training accuracy 0.771104\n",
      "step 17000, train_cost 0.624193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 18001/27000 [4:46:49<2:23:23,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18000, training accuracy 0.772084\n",
      "step 18000, train_cost 0.616047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 19001/27000 [5:02:23<2:07:18,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19000, training accuracy 0.773024\n",
      "step 19000, train_cost 0.607301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 20001/27000 [5:17:58<1:51:16,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20000, training accuracy 0.774014\n",
      "step 20000, train_cost 0.598023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 21001/27000 [5:33:35<1:35:17,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21000, training accuracy 0.774754\n",
      "step 21000, train_cost 0.58832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 22001/27000 [5:49:09<1:19:20,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22000, training accuracy 0.775744\n",
      "step 22000, train_cost 0.578337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 23001/27000 [6:04:44<1:03:24,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23000, training accuracy 0.776334\n",
      "step 23000, train_cost 0.568249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 24001/27000 [6:20:22<47:31,  1.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24000, training accuracy 0.776134\n",
      "step 24000, train_cost 0.558237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 25001/27000 [6:36:28<31:42,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25000, training accuracy 0.776834\n",
      "step 25000, train_cost 0.548477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 26001/27000 [6:52:21<15:50,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26000, training accuracy 0.777354\n",
      "step 26000, train_cost 0.539121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [7:08:10<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy on Test set: 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer4\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"layer4_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"layer4_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h3\" : tf.get_variable(\"layer4_weight_h3\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"layer4_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"layer4_bias_h1\", [1,HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"layer4_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h3\" : tf.get_variable(\"layer4_bias_h3\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"layer4_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weight[\"h3\"]), biases[\"h3\"]))\n",
    "    output = tf.add(tf.matmul(layer3, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/27000 [00:01<10:57:49,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.499965\n",
      "step 0, train_cost 0.718754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1001/27000 [15:40<6:47:05,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, training accuracy 0.602132\n",
      "step 1000, train_cost 0.690741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2001/27000 [31:40<6:35:44,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 0.668357\n",
      "step 2000, train_cost 0.68787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 3001/27000 [47:21<6:18:39,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000, training accuracy 0.71073\n",
      "step 3000, train_cost 0.684972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 4001/27000 [1:03:01<6:02:16,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, training accuracy 0.728051\n",
      "step 4000, train_cost 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 5001/27000 [1:18:41<5:46:11,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000, training accuracy 0.738462\n",
      "step 5000, train_cost 0.678907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 6001/27000 [1:34:22<5:30:14,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, training accuracy 0.743922\n",
      "step 6000, train_cost 0.675647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 7001/27000 [1:50:10<5:14:42,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000, training accuracy 0.750153\n",
      "step 7000, train_cost 0.672173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 8001/27000 [2:06:23<5:00:06,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 0.755103\n",
      "step 8000, train_cost 0.668435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9001/27000 [2:22:31<4:44:59,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000, training accuracy 0.758133\n",
      "step 9000, train_cost 0.664383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10001/27000 [2:38:38<4:29:38,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, training accuracy 0.760073\n",
      "step 10000, train_cost 0.659963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 11001/27000 [2:54:46<4:14:10,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11000, training accuracy 0.761733\n",
      "step 11000, train_cost 0.655121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12001/27000 [3:10:57<3:58:39,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, training accuracy 0.764013\n",
      "step 12000, train_cost 0.649804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 13001/27000 [3:27:25<3:43:21,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13000, training accuracy 0.765254\n",
      "step 13000, train_cost 0.64396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 14001/27000 [3:43:38<3:27:37,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, training accuracy 0.766284\n",
      "step 14000, train_cost 0.637545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 15001/27000 [3:59:35<3:11:38,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15000, training accuracy 0.767094\n",
      "step 15000, train_cost 0.630524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 16001/27000 [4:15:23<2:55:33,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 0.768124\n",
      "step 16000, train_cost 0.62288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 17001/27000 [4:31:07<2:39:27,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17000, training accuracy 0.768874\n",
      "step 17000, train_cost 0.614621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 18001/27000 [4:46:49<2:23:23,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18000, training accuracy 0.769464\n",
      "step 18000, train_cost 0.605785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 19001/27000 [5:02:32<2:07:21,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19000, training accuracy 0.770274\n",
      "step 19000, train_cost 0.596447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 20001/27000 [5:18:15<1:51:22,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20000, training accuracy 0.771474\n",
      "step 20000, train_cost 0.58672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 21001/27000 [5:34:21<1:35:30,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21000, training accuracy 0.772404\n",
      "step 21000, train_cost 0.576755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 22001/27000 [5:50:09<1:19:33,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 22000, training accuracy 0.774024\n",
      "step 22000, train_cost 0.566724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 23001/27000 [6:05:54<1:03:36,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23000, training accuracy 0.774854\n",
      "step 23000, train_cost 0.556806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 24001/27000 [6:21:55<47:43,  1.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24000, training accuracy 0.775914\n",
      "step 24000, train_cost 0.547174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 25001/27000 [6:37:45<31:48,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25000, training accuracy 0.777304\n",
      "step 25000, train_cost 0.537971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 26001/27000 [6:53:34<15:53,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26000, training accuracy 0.778084\n",
      "step 26000, train_cost 0.529306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [7:09:17<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy on Test set: 0.9522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer4\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"layer4_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"layer4_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h3\" : tf.get_variable(\"layer4_weight_h3\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"layer4_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"layer4_bias_h1\", [1,HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"layer4_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h3\" : tf.get_variable(\"layer4_bias_h3\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"layer4_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weight[\"h3\"]), biases[\"h3\"]))\n",
    "    output = tf.add(tf.matmul(layer3, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype double and shape [?,300]\n\t [[Node: Placeholder = Placeholder[dtype=DT_DOUBLE, shape=[?,300], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_2/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_238_Mean_2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-4c56fd9d457c>\", line 13, in <module>\n    input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype double and shape [?,300]\n\t [[Node: Placeholder = Placeholder[dtype=DT_DOUBLE, shape=[?,300], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_2/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_238_Mean_2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype double and shape [?,300]\n\t [[Node: Placeholder = Placeholder[dtype=DT_DOUBLE, shape=[?,300], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_2/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_238_Mean_2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b9ffbb0a72ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# training cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainingOptimzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmergedOP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_vectors\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Write logs at every iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype double and shape [?,300]\n\t [[Node: Placeholder = Placeholder[dtype=DT_DOUBLE, shape=[?,300], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_2/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_238_Mean_2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-4c56fd9d457c>\", line 13, in <module>\n    input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype double and shape [?,300]\n\t [[Node: Placeholder = Placeholder[dtype=DT_DOUBLE, shape=[?,300], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_2/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_238_Mean_2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/log_Layer5\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"layer5_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"layer5_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h3\" : tf.get_variable(\"layer5_weight_h3\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h4\" : tf.get_variable(\"layer5_weight_h4\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"layer5_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"layer5_bias_h1\", [1,HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"layer5_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h3\" : tf.get_variable(\"layer5_bias_h3\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h4\" : tf.get_variable(\"layer5_bias_h4\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"layer5_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weight[\"h3\"]), biases[\"h3\"]))\n",
    "    layer4 = tf.nn.sigmoid(tf.add(tf.matmul(layer3, weight[\"h4\"]), biases[\"h4\"]))\n",
    "    output = tf.add(tf.matmul(layer4, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(3)) :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "# CBOW_HAMvector CBOW_SPMVector  SKIP_GRAM_HAMvector  SKIP_GRAM_SPMVector\n",
    "CBOWTrain = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_HAMvector\",\n",
    "            \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_SPMVector\",]\n",
    "\n",
    "SKIPGRAMTrain = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_HAMvector\",\n",
    "                \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_SPMVector\"]\n",
    "\n",
    "# CBOW_HAMVector  CBOW_SPMVector  SKIP_GRAM_HAMVector  SKIP_GRAM_SPMVector\n",
    "CBOWTest = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_HAMVector\",\n",
    "          \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_SPMVector\",]\n",
    "\n",
    "\n",
    "SKIPGRAMTest = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_HAMVector\",\n",
    "               \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_SPMVector\"]\n",
    "\n",
    "\n",
    "# 0 CBOW, 1 SKIPGRAM\n",
    "VectorType = 1\n",
    "\n",
    "\n",
    "def tsvToNumpyArr(file, delimiterOfDoc):\n",
    "    print (\"Currently, The file(\" + file, \") read\")\n",
    "    return np.genfromtxt(file, delimiter=delimiterOfDoc, dtype=float)\n",
    "\n",
    "def importCBOWData():\n",
    "    CBOWTrainHAM = tsvToNumpyArr(CBOWTrain[0], delimiterOfDoc=\"\\t\")\n",
    "    CBOWtrainSPM = tsvToNumpyArr(CBOWTrain[1], delimiterOfDoc=\"\\t\")\n",
    "    CBOWtestHAM = tsvToNumpyArr(CBOWTest[0], delimiterOfDoc=\"\\t\")\n",
    "    CBOWtestSPM = tsvToNumpyArr(CBOWTest[1], delimiterOfDoc=\"\\t\")\n",
    "    return CBOWTrainHAM, CBOWtrainSPM, CBOWtestHAM, CBOWtestSPM\n",
    "    \n",
    "def importSKIPGRAMData():\n",
    "    SKIPGRAMTrainHAM = tsvToNumpyArr(SKIPGRAMTrain[0], delimiterOfDoc=\"\\t\")\n",
    "    SKIPGRAMtrainSPM = tsvToNumpyArr(SKIPGRAMTrain[1], delimiterOfDoc=\"\\t\")\n",
    "    SKIPGRAMtestHAM = tsvToNumpyArr(SKIPGRAMTest[0], delimiterOfDoc=\"\\t\")\n",
    "    SKIPGRAMtestSPM = tsvToNumpyArr(SKIPGRAMTest[1], delimiterOfDoc=\"\\t\")\n",
    "    return SKIPGRAMTrainHAM, SKIPGRAMtrainSPM, SKIPGRAMtestHAM, SKIPGRAMtestSPM\n",
    "\n",
    "\n",
    "def importData():\n",
    "    if VectorType == 0:\n",
    "        CBOWTrainHAM, CBOWtrainSPM, CBOWtestHAM, CBOWtestSPM = importCBOWData()\n",
    "        return CBOWTrainHAM, CBOWtrainSPM, CBOWtestHAM, CBOWtestSPM\n",
    "    else:\n",
    "        SKIPGRAMTrainHAM, SKIPGRAMtrainSPM, SKIPGRAMtestHAM, SKIPGRAMtestSPM = importSKIPGRAMData()\n",
    "        return SKIPGRAMTrainHAM, SKIPGRAMtrainSPM, SKIPGRAMtestHAM, SKIPGRAMtestSPM\n",
    "    \n",
    "trainHAMX, trainSPMX, testHAMX, testSPMX = importData()\n",
    "\n",
    "print(\"Importing data.... it is done!\")\n",
    "print(\"len of trainHAMX\", len(trainHAMX), \"type of trainHAMX\", type(trainHAMX), \"shape of trainHAMX\", trainHAMX.shape)\n",
    "print(\"len of trainSPMX\", len(trainSPMX), \"type of trainSPMX\", type(trainSPMX), \"shape of trainSPMX\", trainSPMX.shape)\n",
    "print(\"len of testHAMX\", len(testHAMX), \"type of testHAMX\", type(testHAMX), \"shape of testHAMX\", testHAMX.shape)\n",
    "print(\"len of testSPMX\", len(testSPMX), \"type of testSPMX\", type(testSPMX), \"shape of testSPMX\", testSPMX.shape)\n",
    "print()\n",
    "\n",
    "def zerosArr(arr):\n",
    "    return np.zeros((arr.shape[0],1), dtype=float)\n",
    "\n",
    "def onesArr(arr):\n",
    "    return np.ones((arr.shape[0],1), dtype=float)\n",
    "\n",
    "def generateLabel(trainHAMX, trainSPMX, testHAMX, testSPMX):\n",
    "    trainHAMY = np.concatenate((onesArr(trainHAMX), zerosArr(trainHAMX)), axis=1)\n",
    "    trainSPMY = np.concatenate((zerosArr(trainSPMX), onesArr(trainSPMX)), axis=1)\n",
    "    testHAMY = np.concatenate((onesArr(testHAMX), zerosArr(testHAMX)), axis=1)\n",
    "    testSPMY = np.concatenate((zerosArr(testSPMX), onesArr(testSPMX)), axis=1)\n",
    "    \n",
    "    return trainHAMY, trainSPMY, testHAMY, testSPMY\n",
    "    \n",
    "trainHAMY, trainSPMY, testHAMY, testSPMY = generateLabel(trainHAMX, trainSPMX, testHAMX, testSPMX)\n",
    "\n",
    "print(\"len of trainHAMY\", len(trainHAMY), \"type of trainHAMY\", type(trainHAMY), \"shape of trainHAMY\", trainHAMY.shape)\n",
    "print(\"len of trainSPMY\", len(trainSPMY), \"type of trainSPMY\", type(trainSPMY), \"shape of trainSPMY\", trainSPMY.shape)\n",
    "print(\"len of testHAMY\", len(testHAMY), \"type of testHAMY\", type(testHAMY), \"shape of testHAMY\", testHAMY.shape)\n",
    "print(\"len of testSPMY\", len(testSPMY), \"type of testSPMY\", type(testSPMY), \"shape of testSPMY\", testSPMY.shape)\n",
    "print()\n",
    "\n",
    "def concatenateData(trainHAMX, trainSPMX, testHAMX, testSPMX):\n",
    "    trainX = np.concatenate((trainHAMX, trainSPMX), axis=0)\n",
    "    testX = np.concatenate((testHAMX, testSPMX), axis=0)\n",
    "    \n",
    "    return trainX, testX\n",
    "\n",
    "\n",
    "def concatenateLabel(trainHAMY, trainSPMY, testHAMY, testSPMY):\n",
    "    trainY = np.concatenate((trainHAMY, trainSPMY), axis=0)\n",
    "    testY = np.concatenate((testHAMY, testSPMY), axis=0)\n",
    "    \n",
    "    return trainY, testY\n",
    "\n",
    "\n",
    "\n",
    "trainX, testX = concatenateData(trainHAMX, trainSPMX, testHAMX, testSPMX)\n",
    "trainY, testY = concatenateLabel(trainHAMY, trainSPMY, testHAMY, testSPMY)\n",
    "\n",
    "print(trainX)\n",
    "print(trainY)\n",
    "\n",
    "print(\"Finally, basic setting of data is done!!\")\n",
    "\n",
    "print(\"len of trainX\", len(trainX), \"type of trainX\", type(trainX), \"shape of trainX\", trainX.shape)\n",
    "print(\"len of testX\", len(testX), \"type of testX\", type(testX), \"shape of testX\", testX.shape)\n",
    "print(\"len of trainY\", len(trainY), \"type of trainY\", type(trainY), \"shape of trainY\", trainY.shape)\n",
    "print(\"len of testY\", len(testY), \"type of testY\", type(testY), \"shape of trainY\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/skip_gram_log_Layer1\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"skip_gram_Layer1_weight_h1\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"skip_gram_Layer1_bias_h1\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    output = tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"])\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/skip_gram_log_Layer2\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"skip_gram_Layer2_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"skip_gram_Layer2_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"skip_gram_Layer2_bias_h1\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"skip_gram_Layer2_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    output = tf.add(tf.matmul(layer1, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/skip_gram_log_Layer3\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"skip_gram_Layer3_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"skip_gram_Layer3_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"skip_gram_Layer3_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"skip_gram_Layer3_bias_h1\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"skip_gram_Layer3_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"skip_gram_Layer3_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    output = tf.add(tf.matmul(layer2, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/skip_gram_log_Layer4\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"skip_gram_layer4_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"skip_gram_layer4_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h3\" : tf.get_variable(\"skip_gram_layer4_weight_h3\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"skip_gram_layer4_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"skip_gram_layer4_bias_h1\", [1,HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"skip_gram_layer4_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h3\" : tf.get_variable(\"skip_gram_layer4_bias_h3\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"skip_gram_layer4_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weight[\"h3\"]), biases[\"h3\"]))\n",
    "    output = tf.add(tf.matmul(layer3, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "EPOCH = 27000\n",
    "LEARNINGRATE = 0.0008\n",
    "\n",
    "\n",
    "HIDDENLAYERSIZE = trainX.shape[1] # dimension \n",
    "DIMENSION = trainX.shape[1] # demension\n",
    "LABELSIZE = trainY.shape[1] # true or flase\n",
    "\n",
    "logs_path = \"./log/skip_gram_log_Layer5\"\n",
    "\n",
    "# for input of tensorflow \n",
    "input_vectors = tf.placeholder(tf.float64, [None, DIMENSION])\n",
    "ground_truths = tf.placeholder(tf.float64, [None, LABELSIZE])\n",
    "\n",
    "weight = { \"h1\" : tf.get_variable(\"skip_gram_layer5_weight_h1\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h2\" : tf.get_variable(\"skip_gram_layer5_weight_h2\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h3\" : tf.get_variable(\"skip_gram_layer5_weight_h3\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "          \"h4\" : tf.get_variable(\"skip_gram_layer5_weight_h4\", [HIDDENLAYERSIZE, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\": tf.get_variable(\"skip_gram_layer5_weight_output\", [HIDDENLAYERSIZE, 2], dtype=tf.float64)}\n",
    "\n",
    "bias = { \"h1\" : tf.get_variable(\"skip_gram_layer5_bias_h1\", [1,HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h2\" : tf.get_variable(\"skip_gram_layer5_bias_h2\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h3\" : tf.get_variable(\"skip_gram_layer5_bias_h3\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "        \"h4\" : tf.get_variable(\"skip_gram_layer5_bias_h4\", [1, HIDDENLAYERSIZE], dtype=tf.float64),\n",
    "         \"out\" : tf.get_variable(\"skip_gram_layer5_bias_output\", [1, 2], dtype=tf.float64)}\n",
    "\n",
    "def FeedforwardNN(input_vector, weight, biases):\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(input_vectors, weight[\"h1\"]), biases[\"h1\"]))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weight[\"h2\"]), biases[\"h2\"]))\n",
    "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weight[\"h3\"]), biases[\"h3\"]))\n",
    "    layer4 = tf.nn.sigmoid(tf.add(tf.matmul(layer3, weight[\"h4\"]), biases[\"h4\"]))\n",
    "    output = tf.add(tf.matmul(layer4, weight[\"out\"]), bias[\"out\"])\n",
    "    \n",
    "    return output\n",
    "\n",
    "prediction = FeedforwardNN(input_vectors, weight, bias)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ground_truths, logits=prediction))\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "\n",
    "trainingOptimzer = tf.train.GradientDescentOptimizer(learning_rate=LEARNINGRATE).minimize(cost)\n",
    "\n",
    "correctPredictionOP = tf.equal(tf.argmax(prediction,1), tf.argmax(ground_truths,1))\n",
    "accuracyOP = tf.reduce_mean(tf.cast(correctPredictionOP, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracyOP)\n",
    "\n",
    "initOP = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "mergedOP = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initOP)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in tqdm(range(EPOCH)) :\n",
    "        opt, summary =  sess.run([trainingOptimzer, mergedOP], feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "        # Write logs at every iteration\n",
    "        if i % 1000 == 0 : \n",
    "            summary_writer.add_summary(summary, i)\n",
    "            train_accuracy, train_cost = sess.run(\n",
    "                [accuracyOP, cost], \n",
    "                feed_dict={input_vectors : trainX, ground_truths: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, train_cost %g\"%(i, train_cost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracyOP, feed_dict={input_vectors: testX, ground_truths: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1]])\n",
    "b = np.array([[5]])\n",
    "print(np.concatenate((a, b), axis=0))\n",
    "\n",
    "print(np.concatenate((a, b.T), axis=1).shape)b\n",
    "print(1*7+2*9 + 3*11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1.0, 2.0, 3.0],\n",
    "         [4.0, 5.0, 6.0]])\n",
    "\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "print(a +b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1,2,3,4])\n",
    "\n",
    "W = tf.Variable(tf.zeros([2,4]))\n",
    "b = tf.Variable(tf.ones([4]))\n",
    "\n",
    "result = tf.matmul( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
