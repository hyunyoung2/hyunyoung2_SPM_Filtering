{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading /home/hyunyoung2/My_lab/smart_conference/data/Vector/utf8_SKIP_GRAM_Vector .....\n",
      "/home/hyunyoung2/My_lab/smart_conference/data/Vector/utf8_SKIP_GRAM_Vector : 21157\n",
      "Dimensionality of a word vector: 300 \n",
      "\n",
      "making hash function of wordvec....\n",
      "the number of words in a list: 21156\n",
      "reading /home/hyunyoung2/My_lab/smart_conference/data/Text/train/train_20170616-asp-utf8-5000.ham_aa ..... and making sen2vec\n",
      "[('[신한은행]전산시설공사로', '추석(9/19)오전(00~12시)은행업무중지됨을', '안내드립니다.'), ('[olleh]09월', '10일에', '스팸', '차단된', '메시지는', '1건입니다.')]\n",
      "/home/hyunyoung2/My_lab/smart_conference/data/Text/train/train_20170616-asp-utf8-5000.ham_aa : 49993\n",
      "The number of error word: 205002\n",
      "writing /home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_HAMvector to tsv file .....\n",
      "writing is done!\n",
      "\n",
      "\n",
      "\n",
      "reading /home/hyunyoung2/My_lab/smart_conference/data/Text/train/train_20170616-asp-utf8-5000.spm_aa ..... and making sen2vec\n",
      "[('금룡ㆍ', '용왕의', '여위주를', '탐하는', '금룡!', '수문장', '백고래의', '사투ㆍ', 'ks984.com떡값', '7만드림'), ('ㅋㅏ)추운', '날ㅈ', 'l)강원가지', '노)마시고', '집에서', '담배', '피며~', '가입시', '2장증정', 'CV73.COM')]\n",
      "/home/hyunyoung2/My_lab/smart_conference/data/Text/train/train_20170616-asp-utf8-5000.spm_aa : 50000\n",
      "The number of error word: 174977\n",
      "writing /home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_SPMVector to tsv file .....\n",
      "writing is done!\n",
      "\n",
      "\n",
      "\n",
      "reading /home/hyunyoung2/My_lab/smart_conference/data/Text/test/test_HAM-asp-utf8-5000aa ..... and making sen2vec\n",
      "[('마을', '간부회의', '제목:이장건과', '마을', '안건장소:', '마을회관일시:2017년', '1월', '9일', '오후', '7시'), ('장싱크', '아침부터', '계속', '전화해', '전화코드빼낫어', '시간', '나면....')]\n",
      "/home/hyunyoung2/My_lab/smart_conference/data/Text/test/test_HAM-asp-utf8-5000aa : 5000\n",
      "The number of error word: 11715\n",
      "writing /home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_HAMVector to tsv file .....\n",
      "writing is done!\n",
      "\n",
      "\n",
      "\n",
      "reading /home/hyunyoung2/My_lab/smart_conference/data/Text/test/test_SPM-asp-utf8-5000aa ..... and making sen2vec\n",
      "[('[Web발신](광고)1522-', '성공성공청주세종', '오창성공대리운전', '첫', '이용보조', '배터리', '증정수신', '거부', '0802224444'), ('[Web발신]주식전', '종목레버리지', '10배', '1일', '이용료', '1만원,', '이자', 'X매일', '현금', '5만원', '지급수수료', '최저가', 'asian-stock.com')]\n",
      "/home/hyunyoung2/My_lab/smart_conference/data/Text/test/test_SPM-asp-utf8-5000aa : 5000\n",
      "The number of error word: 8982\n",
      "writing /home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_SPMVector to tsv file .....\n",
      "writing is done!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: hyunyoung2\n",
    "\n",
    "# This code make sentence vector from word vector \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for text\n",
    "# path : 0 -> ham, 1 -> spm\n",
    "testPath = [\"/home/hyunyoung2/My_lab/smart_conference/data/Text/test/test_HAM-asp-utf8-5000aa\",\n",
    "         \"/home/hyunyoung2/My_lab/smart_conference/data/Text/test/test_SPM-asp-utf8-5000aa\"]\n",
    "trainPath = [\"/home/hyunyoung2/My_lab/smart_conference/data/Text/train/train_20170616-asp-utf8-5000.ham_aa\",\n",
    "         \"/home/hyunyoung2/My_lab/smart_conference/data/Text/train/train_20170616-asp-utf8-5000.spm_aa\"]\n",
    "\n",
    "# for vector \n",
    "CBOW_testSen2Vec = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_HAMVector\",\n",
    "             \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/CBOW_SPMVector\"]\n",
    "CBOW_trainSen2Vec = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_HAMvector\",\n",
    "              \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/CBOW_SPMVector\"]\n",
    "\n",
    "SKIP_GRAM_testSen2Vec =[\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_HAMVector\",\n",
    "             \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/test/SKIP_GRAM_SPMVector\"]\n",
    "SKIP_GRAM_trainSen2Vec = [\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_HAMvector\",\n",
    "              \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/train/SKIP_GRAM_SPMVector\"]\n",
    "\n",
    "# for vector of whole text\n",
    "wholeVector=[\"/home/hyunyoung2/My_lab/smart_conference/data/Vector/utf8_CBOW_Vector\",\n",
    "            \"/home/hyunyoung2/My_lab/smart_conference/data/Vector/utf8_SKIP_GRAM_Vector\"]\n",
    "\n",
    "# read vector file created from whole text \n",
    "# The first line is the number of words and dimensionality \n",
    "# word vector : hi 1 2 3 1 --> [(hi, 1, 2, 3 , 1), ]\n",
    "def readWholeVectorFile(file):\n",
    "    \n",
    "    wordVec = list ()\n",
    "    \n",
    "    with open(file, \"r\") as rf:\n",
    "        for x in rf.readlines() :\n",
    "            if x == \"\\n\" : \n",
    "                print (\"ERROR-Python : you have the blank line :\", file)\n",
    "                exit()\n",
    "            wordVec.append(tuple(x.split())) \n",
    "            \n",
    "    # To verify\n",
    "    print(file, \":\", len(wordVec))\n",
    "    return wordVec\n",
    "\n",
    "\n",
    "\n",
    "# separate line into word by white space \n",
    "# if you have a doc like \"i am boy\\n, you're girl\\n\" \n",
    "# ----->  [(I, am, boy),(you're,  girl )]\n",
    "def separateLine(file):\n",
    "    return readWholeVectorFile(file)\n",
    "\n",
    "\n",
    "\n",
    "# separate line into word by white space \n",
    "# if you have a doc like \"i am boy\\n, you're girl\\n\" \n",
    "# ----->  [(I, am, boy),(you're,  girl )]\n",
    "def readAFileWithoutNewline(file):\n",
    "    \n",
    "    wordList = list ()\n",
    "    \n",
    "    with open(file, \"r\") as f:\n",
    "        # Because of iconv program, Error a little happens in converting EUC-KR(or CP949) to UTF-8\n",
    "        lineByLine = [x for x in f.readlines() if x != \"\\n\"]    \n",
    "\n",
    "        # Remove Unicode space \n",
    "        for lineIdx, lineStr in enumerate(lineByLine):\n",
    "            wordList.append(tuple(lineStr.split()))\n",
    "    print(wordList[0:2])\n",
    "    print(file, \":\", len(wordList))\n",
    "    return wordList\n",
    "\n",
    "    \n",
    "# Make hash function of words like {hi:[1,2,3,4]. }\n",
    "def hashFunctionOfWords(wordList):\n",
    "    hashWord2Vec = dict()\n",
    "    \n",
    "    print(\"the number of words in a list:\", len(wordList))\n",
    "    \n",
    "    for idx, var in enumerate(wordList):\n",
    "        if hashWord2Vec.get(var[0]) == None:\n",
    "            hashWord2Vec[var[0]] = np.array(list(var[1:]), np.float)\n",
    "        else:\n",
    "            print(\"ERROR-Python, duplication of a word happened\")\n",
    "            exit()\n",
    "    \n",
    "    return hashWord2Vec\n",
    "\n",
    "\n",
    "# Make sentence vector from word2vec using adding. \n",
    "def sen2vec(hashWordVec, lineList, dim):\n",
    "    senVec = list()\n",
    "    errWord = list()\n",
    "    \n",
    "    for lineIdx, lineVar in enumerate(lineList):\n",
    "        senVec.append(np.zeros((dim,), dtype=np.float))\n",
    "        for wordIdx, word in enumerate(lineVar):\n",
    "            try: \n",
    "                senVec[lineIdx] += hashWordVec[word]\n",
    "            except: \n",
    "                errWord.append((lineIdx, word))\n",
    "                \n",
    "    print(\"The number of error word:\", len(errWord))\n",
    "\n",
    "    return senVec\n",
    "# Make TSV flie to utitlize\n",
    "\n",
    "def tsvFile(file, senVec):\n",
    "    print(\"writing\", file, \"to tsv file .....\")\n",
    "    with open(file, \"wb\") as wf:\n",
    "        np.savetxt(wf, senVec,  delimiter=\"\\t\")\n",
    "    print(\"writing is done!\\n\\n\\n\")\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    # first CBOW, Second Skip_gram\n",
    "    # wholeVector = [utf8_CBOW_Vector, utf8_SKIP_GRAM_vector]\n",
    "    for typeIdx, typeName in enumerate(wholeVector):\n",
    "        if typeIdx == 0:\n",
    "            continue\n",
    "        print(\"reading\", typeName, \".....\")\n",
    "        totalWordVec = readWholeVectorFile(typeName)\n",
    "        dim = totalWordVec[0][1]\n",
    "        print(\"Dimensionality of a word vector:\", dim, \"\\n\")\n",
    "        print(\"making hash function of wordvec....\")\n",
    "        hashWord = hashFunctionOfWords(totalWordVec[1:])\n",
    "       \n",
    "    \n",
    "        # train file \n",
    "        for trainIdx, trainFile in enumerate(trainPath):\n",
    "            print(\"reading\", trainFile, \"..... and making sen2vec\")\n",
    "            senVec = sen2vec(hashWord, readAFileWithoutNewline(trainFile), int(dim))\n",
    "            if typeIdx == 0: # CBOW\n",
    "                continue\n",
    "                if trainIdx == 0: # ham\n",
    "                    tsvFile(CBOW_trainSen2Vec[0], senVec)\n",
    "                else: #spm\n",
    "                    tsvFile(CBOW_trainSen2Vec[1], senVec)\n",
    "            else: # Skip gram\n",
    "                if trainIdx == 0: # ham\n",
    "                    tsvFile(SKIP_GRAM_trainSen2Vec[0], senVec)\n",
    "                else: #spm\n",
    "                    tsvFile(SKIP_GRAM_trainSen2Vec[1], senVec)\n",
    "                \n",
    "        # test file  \n",
    "        for testIdx, testFile in enumerate(testPath):\n",
    "            print(\"reading\", testFile, \"..... and making sen2vec\")\n",
    "            senVec = sen2vec(hashWord, readAFileWithoutNewline(testFile), int(dim))\n",
    "            if typeIdx == 0: # CBOW\n",
    "                continue\n",
    "                if testIdx == 0: # ham\n",
    "                    tsvFile(CBOW_testSen2Vec[0], senVec)\n",
    "                else: #spm        \n",
    "                    tsvFile(CBOW_testSen2Vec[1], senVec)\n",
    "            else: # Skip gram\n",
    "                if testIdx == 0: # ham\n",
    "                    tsvFile(SKIP_GRAM_testSen2Vec[0], senVec)\n",
    "                else: #spm\n",
    "                    tsvFile(SKIP_GRAM_testSen2Vec[1], senVec)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ./test to tsv file .....\n",
      "writing is done!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tsvFile(file, senVec):\n",
    "    print(\"reading\", file, \"to tsv file .....\")\n",
    "    with open(file, \"wb\") as wf:\n",
    "        np.savetxt(wf, senVec,  delimiter=\"\\t\")\n",
    "    print(\"writing is done!\\n\\n\\n\")\n",
    "\n",
    "tsvFile(\"./test\", [(1,3),(2,3),(4,324)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n"
     ]
    }
   ],
   "source": [
    "print(\"df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
